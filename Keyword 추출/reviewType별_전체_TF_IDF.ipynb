{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reviewTypeë³„ ì „ì²´ TF-IDF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNdoeCmm0EtKTNyR6CPsAzL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dr-song-summer-project/AI/blob/main/Keyword%20%EC%B6%94%EC%B6%9C/reviewType%EB%B3%84_%EC%A0%84%EC%B2%B4_TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkrpg_ypM0Wr"
      },
      "source": [
        "ë¦¬ë·°íƒ€ì… ë³„ - ë¬¸ì¥ë³„ TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNbY2D4GiUe8",
        "outputId": "7f83b326-e1ef-48fe-cf6a-bcc635ef2699"
      },
      "source": [
        "!pip install konlpy\n",
        "!pip install openpyxl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0Oo-I4Rg8Kt",
        "outputId": "7382ba72-0956-4af3-a0ca-47b4bd66a6bf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgepWG13M6ud"
      },
      "source": [
        "ë¦¬ë·° íƒ€ì… ì „ì²´ êµ°ì§‘ ë³„ ìƒìœ„ í‚¤ì›Œë“œ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqcdTq1HM6VY"
      },
      "source": [
        "from konlpy.tag import Komoran\n",
        "\n",
        "komoran = Komoran()\n",
        "def komoran_tokenize(sent):\n",
        "    words = komoran.pos(sent, join=True)\n",
        "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feJ7FOq_VsHe"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def pagerank(x, df=0.85, max_iter=30, bias=None):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    x : scipy.sparse.csr_matrix\n",
        "        shape = (n vertex, n vertex)\n",
        "    df : float\n",
        "        Damping factor, 0 < df < 1\n",
        "    max_iter : int\n",
        "        Maximum number of iteration\n",
        "    bias : numpy.ndarray or None\n",
        "        If None, equal bias\n",
        "    Returns\n",
        "    -------\n",
        "    R : numpy.ndarray\n",
        "        PageRank vector. shape = (n vertex, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    assert 0 < df < 1\n",
        "\n",
        "    # initialize\n",
        "    A = normalize(x, axis=0, norm='l1')\n",
        "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
        "\n",
        "    # check bias\n",
        "    if bias is None:\n",
        "        bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
        "    else:\n",
        "        bias = bias.reshape(-1,1)\n",
        "        bias = A.shape[0] * bias / bias.sum()\n",
        "        assert bias.shape[0] == A.shape[0]\n",
        "        bias = (1 - df) * bias\n",
        "\n",
        "    # iteration\n",
        "    for _ in range(max_iter):\n",
        "        R = df * (A * R) + bias\n",
        "\n",
        "    return R"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm8ksoKWVuxW"
      },
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "\n",
        "def sent_graph(sents, tokenize=None, min_count=2, min_sim=0.3,\n",
        "    similarity=None, vocab_to_idx=None, verbose=False):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    sents : list of str\n",
        "        Sentence list\n",
        "    tokenize : callable\n",
        "        tokenize(sent) return list of str\n",
        "    min_count : int\n",
        "        Minimum term frequency\n",
        "    min_sim : float\n",
        "        Minimum similarity between sentences\n",
        "    similarity : callable or str\n",
        "        similarity(s1, s2) returns float\n",
        "        s1 and s2 are list of str.\n",
        "        available similarity = [callable, 'cosine', 'textrank']\n",
        "    vocab_to_idx : dict\n",
        "        Vocabulary to index mapper.\n",
        "        If None, this function scan vocabulary first.\n",
        "    verbose : Boolean\n",
        "        If True, verbose mode on\n",
        "    Returns\n",
        "    -------\n",
        "    sentence similarity graph : scipy.sparse.csr_matrix\n",
        "        shape = (n sents, n sents)\n",
        "    \"\"\"\n",
        "\n",
        "    if vocab_to_idx is None:\n",
        "        idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
        "    else:\n",
        "        idx_to_vocab = [vocab for vocab, _ in sorted(vocab_to_idx.items(), key=lambda x:x[1])]\n",
        "\n",
        "    x = vectorize_sents(sents, tokenize, vocab_to_idx)\n",
        "    if similarity == 'cosine':\n",
        "        x = numpy_cosine_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
        "    else:\n",
        "        x = numpy_textrank_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
        "    return x\n",
        "\n",
        "def vectorize_sents(sents, tokenize, vocab_to_idx):\n",
        "    rows, cols, data = [], [], []\n",
        "    for i, sent in enumerate(sents):\n",
        "        counter = Counter(tokenize(sent))\n",
        "        for token, count in counter.items():\n",
        "            j = vocab_to_idx.get(token, -1)\n",
        "            if j == -1:\n",
        "                continue\n",
        "            rows.append(i)\n",
        "            cols.append(j)\n",
        "            data.append(count)\n",
        "    n_rows = len(sents)\n",
        "    n_cols = len(vocab_to_idx)\n",
        "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
        "\n",
        "def numpy_cosine_similarity_matrix(x, min_sim=0.3, verbose=True, batch_size=1000):\n",
        "    n_rows = x.shape[0]\n",
        "    mat = []\n",
        "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
        "        b = int(bidx * batch_size)\n",
        "        e = min(n_rows, int((bidx+1) * batch_size))\n",
        "        psim = 1 - pairwise_distances(x[b:e], x, metric='cosine')\n",
        "        rows, cols = np.where(psim >= min_sim)\n",
        "        data = psim[rows, cols]\n",
        "        mat.append(csr_matrix((data, (rows, cols)), shape=(e-b, n_rows)))\n",
        "        if verbose:\n",
        "            print('\\rcalculating cosine sentence similarity {} / {}'.format(b, n_rows), end='')\n",
        "    mat = sp.sparse.vstack(mat)\n",
        "    if verbose:\n",
        "        print('\\rcalculating cosine sentence similarity was done with {} sents'.format(n_rows))\n",
        "    return mat\n",
        "\n",
        "def numpy_textrank_similarity_matrix(x, min_sim=0.3, verbose=True, min_length=1, batch_size=1000):\n",
        "    n_rows, n_cols = x.shape\n",
        "\n",
        "    # Boolean matrix\n",
        "    rows, cols = x.nonzero()\n",
        "    data = np.ones(rows.shape[0])\n",
        "    z = csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
        "\n",
        "    # Inverse sentence length\n",
        "    size = np.asarray(x.sum(axis=1)).reshape(-1)\n",
        "    size[np.where(size <= min_length)] = 10000\n",
        "    size = np.log(size)\n",
        "\n",
        "    mat = []\n",
        "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
        "\n",
        "        # slicing\n",
        "        b = int(bidx * batch_size)\n",
        "        e = min(n_rows, int((bidx+1) * batch_size))\n",
        "\n",
        "        # dot product\n",
        "        inner = z[b:e,:] * z.transpose()\n",
        "\n",
        "        # sentence len[i,j] = size[i] + size[j]\n",
        "        norm = size[b:e].reshape(-1,1) + size.reshape(1,-1)\n",
        "        norm = norm ** (-1)\n",
        "        norm[np.where(norm == np.inf)] = 0\n",
        "\n",
        "        # normalize\n",
        "        sim = inner.multiply(norm).tocsr()\n",
        "        rows, cols = (sim >= min_sim).nonzero()\n",
        "        data = np.asarray(sim[rows, cols]).reshape(-1)\n",
        "\n",
        "        # append\n",
        "        mat.append(csr_matrix((data, (rows, cols)), shape=(e-b, n_rows)))\n",
        "\n",
        "        if verbose:\n",
        "            print('\\rcalculating textrank sentence similarity {} / {}'.format(b, n_rows), end='')\n",
        "\n",
        "    mat = sp.sparse.vstack(mat)\n",
        "    if verbose:\n",
        "        print('\\rcalculating textrank sentence similarity was done with {} sents'.format(n_rows))\n",
        "\n",
        "    return mat\n",
        "\n",
        "def graph_with_python_sim(tokens, verbose, similarity, min_sim):\n",
        "    if similarity == 'cosine':\n",
        "        similarity = cosine_sent_sim\n",
        "    elif callable(similarity):\n",
        "        similarity = similarity\n",
        "    else:\n",
        "        similarity = textrank_sent_sim\n",
        "\n",
        "    rows, cols, data = [], [], []\n",
        "    n_sents = len(tokens)\n",
        "    for i, tokens_i in enumerate(tokens):\n",
        "        if verbose and i % 1000 == 0:\n",
        "            print('\\rconstructing sentence graph {} / {} ...'.format(i, n_sents), end='')\n",
        "        for j, tokens_j in enumerate(tokens):\n",
        "            if i >= j:\n",
        "                continue\n",
        "            sim = similarity(tokens_i, tokens_j)\n",
        "            if sim < min_sim:\n",
        "                continue\n",
        "            rows.append(i)\n",
        "            cols.append(j)\n",
        "            data.append(sim)\n",
        "    if verbose:\n",
        "        print('\\rconstructing sentence graph was constructed from {} sents'.format(n_sents))\n",
        "    return csr_matrix((data, (rows, cols)), shape=(n_sents, n_sents))\n",
        "\n",
        "def textrank_sent_sim(s1, s2):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    s1, s2 : list of str\n",
        "        Tokenized sentences\n",
        "    Returns\n",
        "    -------\n",
        "    Sentence similarity : float\n",
        "        Non-negative number\n",
        "    \"\"\"\n",
        "    n1 = len(s1)\n",
        "    n2 = len(s2)\n",
        "    if (n1 <= 1) or (n2 <= 1):\n",
        "        return 0\n",
        "    common = len(set(s1).intersection(set(s2)))\n",
        "    base = math.log(n1) + math.log(n2)\n",
        "    return common / base\n",
        "\n",
        "def cosine_sent_sim(s1, s2):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    s1, s2 : list of str\n",
        "        Tokenized sentences\n",
        "    Returns\n",
        "    -------\n",
        "    Sentence similarity : float\n",
        "        Non-negative number\n",
        "    \"\"\"\n",
        "    if (not s1) or (not s2):\n",
        "        return 0\n",
        "\n",
        "    s1 = Counter(s1)\n",
        "    s2 = Counter(s2)\n",
        "    norm1 = math.sqrt(sum(v ** 2 for v in s1.values()))\n",
        "    norm2 = math.sqrt(sum(v ** 2 for v in s2.values()))\n",
        "    prod = 0\n",
        "    for k, v in s1.items():\n",
        "        prod += v * s2.get(k, 0)\n",
        "    return prod / (norm1 * norm2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1x0pLGCSG-w"
      },
      "source": [
        "import numpy as np\n",
        "class KeywordSummarizer:\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    sents : list of str\n",
        "        Sentence list\n",
        "    tokenize : callable\n",
        "        Tokenize function: tokenize(str) = list of str\n",
        "    min_count : int\n",
        "        Minumum frequency of words will be used to construct sentence graph\n",
        "    window : int\n",
        "        Word cooccurrence window size. Default is -1.\n",
        "        '-1' means there is cooccurrence between two words if the words occur in a sentence\n",
        "    min_cooccurrence : int\n",
        "        Minimum cooccurrence frequency of two words\n",
        "    vocab_to_idx : dict or None\n",
        "        Vocabulary to index mapper\n",
        "    df : float\n",
        "        PageRank damping factor\n",
        "    max_iter : int\n",
        "        Number of PageRank iterations\n",
        "    verbose : Boolean\n",
        "        If True, it shows training progress\n",
        "    \"\"\"\n",
        "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
        "        window=-1, min_cooccurrence=2, vocab_to_idx=None,\n",
        "        df=0.85, max_iter=30, verbose=False):\n",
        "\n",
        "        self.tokenize = tokenize\n",
        "        self.min_count = min_count\n",
        "        self.window = window\n",
        "        self.min_cooccurrence = min_cooccurrence\n",
        "        self.vocab_to_idx = vocab_to_idx\n",
        "        self.df = df\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if sents is not None:\n",
        "            self.train_textrank(sents)\n",
        "\n",
        "    def train_textrank(self, sents, bias=None):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        sents : list of str\n",
        "            Sentence list\n",
        "        bias : None or numpy.ndarray\n",
        "            PageRank bias term\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "\n",
        "        g, self.idx_to_vocab = word_graph(sents,\n",
        "            self.tokenize, self.min_count,self.window,\n",
        "            self.min_cooccurrence, self.vocab_to_idx, self.verbose)\n",
        "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
        "        if self.verbose:\n",
        "            print('trained TextRank. n words = {}'.format(self.R.shape[0]))\n",
        "\n",
        "    def keywords(self, topk=30):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        topk : int\n",
        "            Number of keywords selected from TextRank\n",
        "        Returns\n",
        "        -------\n",
        "        keywords : list of tuple\n",
        "            Each tuple stands for (word, rank)\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'R'):\n",
        "            raise RuntimeError('Train textrank first or use summarize function')\n",
        "        idxs = self.R.argsort()[-topk:]\n",
        "        keywords = [(self.idx_to_vocab[idx], self.R[idx]) for idx in reversed(idxs)]\n",
        "        return keywords\n",
        "\n",
        "    def summarize(self, sents, topk=30):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        sents : list of str\n",
        "            Sentence list\n",
        "        topk : int\n",
        "            Number of keywords selected from TextRank\n",
        "        Returns\n",
        "        -------\n",
        "        keywords : list of tuple\n",
        "            Each tuple stands for (word, rank)\n",
        "        \"\"\"\n",
        "\n",
        "        self.train_textrank(sents)\n",
        "        return self.keywords(topk)\n",
        "\n",
        "        \n",
        "\n",
        "class KeysentenceSummarizer:\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    sents : list of str\n",
        "        Sentence list\n",
        "    tokenize : callable\n",
        "        Tokenize function: tokenize(str) = list of str\n",
        "    min_count : int\n",
        "        Minumum frequency of words will be used to construct sentence graph\n",
        "    min_sim : float\n",
        "        Minimum similarity between sentences in sentence graph\n",
        "    similarity : str\n",
        "        available similarity = ['cosine', 'textrank']\n",
        "    vocab_to_idx : dict or None\n",
        "        Vocabulary to index mapper\n",
        "    df : float\n",
        "        PageRank damping factor\n",
        "    max_iter : int\n",
        "        Number of PageRank iterations\n",
        "    verbose : Boolean\n",
        "        If True, it shows training progress\n",
        "    \"\"\"\n",
        "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
        "        min_sim=0.3, similarity=None, vocab_to_idx=None,\n",
        "        df=0.85, max_iter=30, verbose=False):\n",
        "\n",
        "        self.tokenize = tokenize\n",
        "        self.min_count = min_count\n",
        "        self.min_sim = min_sim\n",
        "        self.similarity = similarity\n",
        "        self.vocab_to_idx = vocab_to_idx\n",
        "        self.df = df\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if sents is not None:\n",
        "            self.train_textrank(sents)\n",
        "\n",
        "    def train_textrank(self, sents, bias=None):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        sents : list of str\n",
        "            Sentence list\n",
        "        bias : None or numpy.ndarray\n",
        "            PageRank bias term\n",
        "            Shape must be (n_sents,)\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        g = sent_graph(sents, self.tokenize, self.min_count,\n",
        "            self.min_sim, self.similarity, self.vocab_to_idx, self.verbose)\n",
        "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
        "        if self.verbose:\n",
        "            print('trained TextRank. n sentences = {}'.format(self.R.shape[0]))\n",
        "\n",
        "    def summarize(self, sents, topk=30, bias=None):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        sents : list of str\n",
        "            Sentence list\n",
        "        topk : int\n",
        "            Number of key-sentences to be selected.\n",
        "        bias : None or numpy.ndarray\n",
        "            PageRank bias term\n",
        "            Shape must be (n_sents,)\n",
        "        Returns\n",
        "        -------\n",
        "        keysents : list of tuple\n",
        "            Each tuple stands for (sentence index, rank, sentence)\n",
        "        Usage\n",
        "        -----\n",
        "            >>> from textrank import KeysentenceSummarizer\n",
        "            >>> summarizer = KeysentenceSummarizer(tokenize = tokenizer, min_sim = 0.5)\n",
        "            >>> keysents = summarizer.summarize(texts, topk=30)\n",
        "        \"\"\"\n",
        "        n_sents = len(sents)\n",
        "        if isinstance(bias, np.ndarray):\n",
        "            if bias.shape != (n_sents,):\n",
        "                raise ValueError('The shape of bias must be (n_sents,) but {}'.format(bias.shape))\n",
        "        elif bias is not None:\n",
        "            raise ValueError('The type of bias must be None or numpy.ndarray but the type is {}'.format(type(bias)))\n",
        "\n",
        "        self.train_textrank(sents, bias)\n",
        "        idxs = self.R.argsort()[-topk:]\n",
        "        try:\n",
        "          keysents = [(idx, self.R[idx], sents[idx]) for idx in reversed(idxs)]\n",
        "        except:\n",
        "          print(sents)\n",
        "          keysents = False\n",
        "\n",
        "        return keysents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi3tL7kUV0WX"
      },
      "source": [
        "from collections import Counter\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def scan_vocabulary(sents, tokenize=None, min_count=2):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    sents : list of str\n",
        "        Sentence list\n",
        "    tokenize : callable\n",
        "        tokenize(str) returns list of str\n",
        "    min_count : int\n",
        "        Minumum term frequency\n",
        "    Returns\n",
        "    -------\n",
        "    idx_to_vocab : list of str\n",
        "        Vocabulary list\n",
        "    vocab_to_idx : dict\n",
        "        Vocabulary to index mapper.\n",
        "    \"\"\"\n",
        "    counter = Counter(w for sent in sents for w in tokenize(sent))\n",
        "    counter = {w:c for w,c in counter.items() if c >= min_count}\n",
        "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
        "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
        "    return idx_to_vocab, vocab_to_idx\n",
        "\n",
        "def tokenize_sents(sents, tokenize):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    sents : list of str\n",
        "        Sentence list\n",
        "    tokenize : callable\n",
        "        tokenize(sent) returns list of str (word sequence)\n",
        "    Returnsí”¼ì„\n",
        "    -------\n",
        "    tokenized sentence list : list of list of str\n",
        "    \"\"\"\n",
        "    return [tokenize(sent) for sent in sents]\n",
        "\n",
        "def vectorize(tokens, vocab_to_idx):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    tokens : list of list of str\n",
        "        Tokenzed sentence list\n",
        "    vocab_to_idx : dict\n",
        "        Vocabulary to index mapper\n",
        "    Returns\n",
        "    -------\n",
        "    sentence bow : scipy.sparse.csr_matrix\n",
        "        shape = (n_sents, n_terms)\n",
        "    \"\"\"\n",
        "    rows, cols, data = [], [], []\n",
        "    for i, tokens_i in enumerate(tokens):\n",
        "        for t, c in Counter(tokens_i).items():\n",
        "            j = vocab_to_idx.get(t, -1)\n",
        "            if j == -1:\n",
        "                continue\n",
        "            rows.append(i)\n",
        "            cols.append(j)\n",
        "            data.append(c)\n",
        "    n_sents = len(tokens)\n",
        "    n_terms = len(vocab_to_idx)\n",
        "    x = csr_matrix((data, (rows, cols)), shape=(n_sents, n_terms))\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2nygRz_NFEG",
        "outputId": "3fb74091-9134-4b5c-e5e0-e74fe0a966e4"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "text = 'ì•ˆë…•í•˜ì„¸ìš” ë°˜ê°‘ìŠµë‹ˆë‹¤ğŸ¶'\n",
        "print(text) \n",
        "only_BMP_pattern = re.compile(\"[\"\n",
        "        u\"\\U00010000-\\U0010FFFF\"  #BMP characters ì´ì™¸\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "path = '/content/drive/My Drive/ë‹¥í„°ì†¡ ì—¬ë¦„ í”„ë¡œì íŠ¸/4. ëŒ€-ìŠ¤íƒ€ í•´ê²° 2/ë°ì´í„°/unlabeled_data_excel.csv'\n",
        "df = pd.read_csv(path)\n",
        "data = pd.DataFrame.to_numpy(df)\n",
        "\n",
        "\n",
        "test = [[] for _ in range(3)]\n",
        "idx = 0\n",
        "for content in data:\n",
        "  if content[5] == 'recruitReview':\n",
        "    test[2].append(only_BMP_pattern.sub(r'', content[3]))\n",
        "  elif content[5] == 'interviewReview':\n",
        "    test[0].append(only_BMP_pattern.sub(r'', content[3]))\n",
        "  else:\n",
        "    test[1].append(only_BMP_pattern.sub(r'', content[3]))\n",
        "\n",
        "print(test[0][0], test[1][0], test[2][0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì•ˆë…•í•˜ì„¸ìš” ë°˜ê°‘ìŠµë‹ˆë‹¤ğŸ¶\n",
            "ì•ˆë…•í•˜ì„¸ìš” ë°˜ê°‘ìŠµë‹ˆë‹¤\n",
            "ìˆ˜ë½í•˜ì‹œê³  ì „í™” ì¸í„°ë·° ì§„í–‰í•´ì£¼ì…¨ê³ ìš”. ì²«ì§¸ ì•„ì´ ëŒë´„ êµ¬ì¸ ê¸€ì„ ë³´ê³  ì§€ì›ì„ í•˜ì˜€ëŠ”ë°, ì „í™” ì¸í„°ë·° ì§„í–‰í•˜ì‹¤ ë•Œ ë‘˜ì§¸ ì•„ì´ ë°©í•™ì´ ë‹¤ê°€ì˜¤ëŠ” ë° ê·¸ë•Œë„ ê´œì°®ëƒê³  ë¬¼ì–´ë³´ì…¨ì§€ë§Œ, ë‘˜ì§¸ ì•„ì´ê°€ ë°©í•™í•˜ëŠ” ì£¼ì—ëŠ” ì„ ì•½ ëŒë´„ ì§‘ë“¤ì´ ìˆë‹¤ê³  ë§ì”€ë“œë¦¬ë‹ˆ, ì²«ì§¸ ì•„ì´ì™€ ë‘˜ì§¸ ì•„ì´ë¥¼ ê°™ì´ ëŒë³¼ ìˆ˜ ìˆëŠ” ë¶„ì„ ì°¾ìœ¼ì‹ ë‹¤ê³  í•˜ì…”ì„œ ì•„ì‰½ê²Œë„ ëµ™ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì•„ì´ë“¤ê³¼ ìƒˆí•´ì—ë„ í–‰ë³µí•˜ì„¸ìš”. ì—°ë½ì´ ì˜¤ì§€ ì•Šì•„ ëµ™ì§€ ëª»í•˜ì˜€ìŠµë‹ˆë‹¤ ëŒë³´ì‹œë‹¤ê°€ ì‹œê°„ì´ ë§ì§€ ì•Šìœ¼ì…¨ëŠ”ì§€ ê³§ ê·¸ë§Œë‘ì‹  ë‹¤ í•˜ì…¨ìŠµë‹ˆë‹¤. ì¼í•˜ì‹œë‹¤ ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ì‹  ë¶€ë¶„ì„ ë‚˜ë¦„ ì¡°ìœ¨í•´ ë“œë¦¬ë ¤ í–ˆìœ¼ë‚˜, ë‹¤ë¥¸ ì¡°ê±´ ì¡°ìœ¨ ê³¼ì • ì—†ì´ ë°”ë¡œ ê·¸ë§Œë‘ì‹œê² ë‹¤ í•˜ì…”ì„œ ì›Œí‚¹ë§˜ ì…ì¥ì—ì„œ ë§¤ìš° ë‚œì²˜í–ˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POZHtVe2NJ5R",
        "outputId": "e1659689-ea1a-4062-f96c-4c9904564bef"
      },
      "source": [
        "summarizer = KeysentenceSummarizer(\n",
        "    tokenize = komoran_tokenize,\n",
        "    min_sim = 0.5,\n",
        "    verbose = True\n",
        ")\n",
        "\n",
        "keysents = summarizer.summarize(test[2], topk=10)\n",
        "for sent_idx, rank, sent in keysents:  \n",
        "  print(f'{sent_idx} : {rank} :: {sent}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calculating textrank sentence similarity was done with 3321 sents\n",
            "trained TextRank. n sentences = 3321\n",
            "1808 : 5.2517252145090225 :: ì•„ì´ê°€ ì²«ì§¸ ë‚ ì—ëŠ” ë‚¯ê°€ë¦¼ë„ í•˜ê³  ì ë‹¤ ê¹¬ì§€ ì–¼ë§ˆ ì•ˆ ë¼ì„œ ì íˆ¬ì •ì´ ìˆì—ˆë˜ ê±´ì§€ ê·¸ë‚ ì€ ë‚´ë‚´ ê¸°ë¶„ì´ ì¢‹ë‹¤ê°€ë„ ê¸ˆë°© ë‹¤ì‹œ ì—„ë§ˆë¥¼ ì°¾ì•˜ëŠ”ë°, ë‘˜ì§¸ ë‚ ì€ ì–¼êµ´ ë³´ìë§ˆì ì›ƒì–´ì£¼ê³  ì¸ì‚¬ë„ í•´ì¤¬ìŠµë‹ˆë‹¤. ëŒë´ì£¼ëŠ” ë‚´ë‚´ ê¸°ë¶„ ì¢‹ì•„ì„œ ì˜ ì›ƒê³  ë–¼ë¥¼ ì“°ì§€ë„ ì•Šì•˜ìŠµë‹ˆë‹¤. ì•„ê¸°ê°€ ì—ë„ˆì§€ê°€ ë„˜ì¹˜ê³  ìê¸°ì£¼ì¥ì´ í™•ì‹¤í•œ í¸ì´ë¼ ìê¸°ê°€ ë¬´ì–¼ ì›í•˜ëŠ”ì§€ ë¶„ëª…í•˜ê²Œ ë§í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ì•„ì´ê°€ ì§€ ê¸ˆ ì›í•˜ëŠ” í•´ì¤„ ìˆ˜ ìˆì–´ì„œ ì €ëŠ” ë§ˆìŒì´ ì¡°ê¸ˆ í¸í–ˆì–´ìš”. ì–´ë¨¸ë‹˜ë„ êµ‰ì¥íˆ ìœ ì¾Œí•˜ì…”ì„œ ê·¸ëŸ°ì§€ ì•„ê¸°ë„ êµ‰ì¥íˆ ì˜ ì›ƒê³  ì• ì •ì„ ë‚˜ëˆ„ëŠ” ë°©ë²•ì„ ì•Œê³  ìˆëŠ” ê²ƒ ê°™ì•„ìš”. í™œë™ë¹„ëŠ” ì œê°€ í¸í•˜ê²Œ ë§ˆì§€ë§‰ ë‚ ì— ë°›ê² ë‹¤ê³  í–ˆê³  ë´‰íˆ¬ì— ê°ì‚¬í•˜ê²Œ ê¸€ê¹Œì§€ ì ì–´ì„œ ì£¼ì…¨ì–´ìš”. í™œë™ë„ ë¯¸ë¦¬ ë§ì”€í•´ ì£¼ì‹  ê²ƒ ì™¸ì— ë”°ë¡œ ì‹œí‚¤ì‹  ê²ƒë„ ì—†ì—ˆê³  ì‹œê°„ë„ ì •í™•í•˜ê²Œ ì§€ì¼œì£¼ì…¨ìŠµë‹ˆë‹¤~ ì €ëŠ” ì‚¬ ì •ìƒ ì €ë…ì—ëŠ” ì‹œê°„ ë‚´ê¸°ê°€ ì–´ë ¤ì›Œ ê³„ì† ëŒë´ì¤„ ìˆ˜ëŠ” ì—†ì—ˆì§€ë§Œ ì¢‹ì€ ë¶„ ë§Œë‚˜ì…¨ìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤\n",
            "904 : 4.940581320667702 :: ì§‘ì— 13ê°œì›” ì•„ê°€ì™€ 5ì‚´ ë‚¨ì•„ê°€ ìˆëŠ” ì§‘ì…ë‹ˆë‹¤. 13ê°œì›” ì•„ê°€ëŠ” ì œê°€ ì¼€ì–´í•˜ê³  5ì‚´ ë‚¨ì•„ì˜ í•˜ì› ë° ëŒë´„ì´ 2ì£¼ ë™ì•ˆ(9ì¼) í•„ìš”í•´ì„œ ì‹ ì²­ë“œë ¸ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ 4ì¼ ë™ì•ˆ ì•„ì´ë¥¼ ë´ì£¼ì‹œëŠ” ì¤‘ì— 2ì¼ì€ 10ë¶„ ì´ìƒ ì§€ê°í•˜ì…¨ê³  ì•„ì´ê°€ í™”ì¥ì‹¤ì„ ê°ˆ ë•Œë„ ì €ì—ê²Œ ë§¡ê¸°ì‹œë”êµ°ìš”. ê·¸ë¦¬ê³  ì¤‘ê°„ì— ëŒë´„ì„ ê·¸ë§Œë‘” ê°€ì¥ í° ì´ìœ ëŠ” ì•„ì´ ì €ë… ë•Œë¬¸ì…ë‹ˆë‹¤. ëŒë´„ ì‹œê°„ì´ 4 ì‹œì—ì„œ 7ì‹œì˜€ëŠ”ë° ì•„ì´ë“¤ ì €ë… ì´ ì• ë§¤í•´ì„œ 13ê°œì›” ì•„ì´ì™€ 5ì‚´ ì•„ì´ë¥¼ ê°™ì´ ë¨¹ì´ê³  ì²˜ìŒì—ëŠ” ì‹œí„° ë¶„ë„ ì €ë…ì„ ê°™ì´ ì œê³µí•´ë“œë ¸ìŠµë‹ˆë‹¤. ê·¸ëŸ¼ 5ì‚´ ì•„ì´ëŠ” ì €ë…ì„ ë¨¹ì—¬ì£¼ì‹œê±°ë‚˜ ì¼€ì–´í•´ ì£¼ì…”ì•¼ í•˜ëŠ”ë° í˜¼ì ë§›ìˆê²Œ 40ì—¬ ë¶„ ë™ì•ˆ ë“œì‹œë”êµ°ìš”. ì´í‹€ ì €ë…ì„ ë“œë ¸ì—ˆëŠ”ë° ë³€í•˜ì§€ë¥¼ ì•Šì•„ì„œ ë‚˜ë¨¸ì§€ ì´í‹€ì€ ì±™ê²¨ë“œë¦¬ì§€ ì•Šì•˜ì–´ìš”. ì œê°€ í˜¼ì ì•„ê°€ ë‘˜ì„ ì¼€ì–´í•˜ê¸°ê°€ ì¢€ í˜ë“¤ë”ë¼ê³ ìš”. ë†€ì´ëŠ” ì•„ì´ì— ë§ì¶°ì„œ ë†€ì•„ì£¼ì…¨ì–´ìš”. ì„±ê²©ì€ ë°ê³  ì¢‹ìœ¼ì‹  ë“¯ í•˜ë‚˜ ì±…ì„ê°ì´ ìˆì–´ ë³´ì´ì§€ëŠ” ì•Šì•˜ìŠµë‹ˆë‹¤. ë†€ì´ ì œì™¸í•˜ê³ ëŠ” í‰ê°€ë¥¼ ì¢‹ê²Œ ë“œë¦´ ìˆ˜ê°€ ì—†ì„ ê²ƒ ê°™ì•„ìš”.\n",
            "822 : 4.756238652521562 :: ì²˜ìŒì´ë¼ ì‹œì§€ë§Œ, ê¸°ë³¸ì ìœ¼ë¡œ ì•„ê¸°ë¥¼ ë„ˆë¬´ë„ˆë¬´ ëª» ë³´ì…¨ê³ ... ì•„ê¸°ì— ëŒ€í•´ ì˜ ëª¨ë¥´ì…¨ì–´ìš” ã… ã…  5ì‹œê°„ë§Œ í•„ìš”í–ˆëŠ”ë°, ì–´ì©” ìˆ˜ ì—†ì´ 10ì‹œê°„ í’€íƒ€ì„ìœ¼ë¡œ ì±„ìš©í–ˆì§€ë§Œ ì²­ì†Œ ë“± ë„ì™€ì£¼ì‹œê¸°ë¡œ í•´ì„œ ë„ˆë¬´ë‚˜ ê°ì‚¬í–ˆì–´ìš” í•˜ì§€ë§Œ ì•„ì´ë¥¼ ëŒë³´ëŠ” ì¼ë³´ë‹¤ ê¸°íƒ€ ê°€ì‚¬ë‚˜ ë³¸ì¸ì˜ íœ´ê²Œì‹œê°„ì„ ë” ì±™ê¸°ì‹œëŠ” ê²ƒ ê°™ì•˜ê³  ì•„ì´ë¥¼ ì˜ ëª» ë³´ì…”ì„œ ì‹œí„°ë¥¼ ì“°ë©´ì„œë„ ë‚´ë‚´ ë¶ˆí¸í–ˆë„¤ìš”... ê°€ì„œ ì¼ì„ ë„ì™€ì£¼ì‹œëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ ì‹œí„°ë¼ë©´ ë¬´ì—‡ë³´ë‹¤ ì•„ê¸°ë¥¼ ì˜ ë´ì£¼ì‹œëŠ” ê²Œ ê°€ì¥ ì¤‘ìš”í•  ê²ƒ ê°™ì•„ ìš” ì‹œê°„, ìœ„ìƒ, ì•ˆ ì „ë©´ì—ì„œ ì¢€ ë” ì‹ ê²½ ì“°ì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”. ì €í¬ì™€ëŠ” ì˜ ë§ì§€ ì•Šì•˜ì§€ë§Œ ì›Œë‚™ ì°©í•˜ì‹  ë¶„ì´ë¼ ë‹¤ë¥¸ ì§‘ì—ì„œëŠ” ì˜ ì§€ ë‚´ì‹¤ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”. ê·¸ë™ì•ˆ ê³ ìƒ ë§ìœ¼ì…¨ìŠµë‹ˆë‹¤~\n",
            "2894 : 4.670203665840344 :: ê²½ë ¥ì´ ìˆìœ¼ì…”ì„œ ê·¸ëŸ°ì§€ ëŠ¥ìˆ™í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì•„ì´ë¥¼ ì˜ ëŒë´ì£¼ì…¨ì–´ìš”. ì‹­ë¶„ ì¼ì° ì˜¤ì…”ì„œ ë°¥ ë¨¹ì´ê¸°, ì¬ìš°ê¸°, ëª©ìš•ì‹œí‚¤ê¸°, ë†€ì•„ì£¼ê¸°, ì•ˆ í•´ë„ ë˜ëŠ” ê°„ë‹¨ ì²­ì†Œ ì„¤ê±°ì§€ê¹Œì§€ ì œê°€ ì‹ ê²½ ì“¸ í•„ìš” ì—†ê²Œ ì²™ì²™ ì˜ í•´ì£¼ì…”ì„œ ì´ì‚¬ ì¤€ë¹„í•˜ëŠë¼ ì •ì‹ ì—†ë˜ ì €ëŠ” ë§¤ìš° ë“ ë“ í–ˆë‹µë‹ˆë‹¤. íŠ¹íˆ ì•„ê¸°ê°€ ëª¨ì„¸ ê¸°ê´€ì§€ì—¼ ì™€ì„œ ì•ˆ ë¨¹ê³  ì•ˆì ê³  ë³´ì±˜ëŠ”ë° ì‹œí„°ë‹˜ê»˜ì„œ ì˜¤ë˜ë„ë¡ ì•ˆì•„ì„œ ì‰´ ìˆ˜ ìˆê²Œ í•´ì£¼ì‹œê³  ì»¨ë””ì…˜ íšŒë³µí•˜ë„ë¡ ì‹ ê²½ ì¨ì„œ ë¬¼ ìì£¼ ë¨¹ì—¬ì£¼ì‹œê³  ë°¥ë„ ëˆê¸° ìˆê²Œ ë¨¹ì—¬ì£¼ì‹œê³  í–ˆë˜ ê²ƒ ê¸°ì–µì— ë‚¨ìŠµë‹ˆë‹¤. ë•ë¶„ì— ì•„ê¸° ê±´ê°• ì˜ íšŒë³µí•˜ê³  ì´ì‚¬ë„ ë¬´ì‚¬íˆ í–ˆë˜ ê²ƒ ê°™ì•„ ê°ì‚¬í•œ ë§ˆìŒì…ë‹ˆë‹¤. ì§§ì€ ì‹œê°„ì´ì—ˆì§€ë§Œ ì¢‹ì€ ì¸ì—°ì´ì—ˆë˜ ê²ƒ ê°™ì•„ ì˜¤ë˜ë„ë¡ ê¸°ì–µì— ë‚¨ì„ ê²ƒ ê°™ê³  ë‹¤ìŒì— ë˜ ëµ ê¸°íšŒê°€ ìˆì—ˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤\n",
            "492 : 4.303652858152414 :: ì €í¬ ì•„ì´ì˜ ëˆˆ ë†’ì´ ë§ê²Œ ì†Œê¿‰ë†€ì´ë¥¼ ì˜í•´ì¤¬ê³  ì•„ì´ê°€ ì„ ìƒë‹˜ ì˜¤ì‹œëŠ” ê±° ë„ˆë¬´ ì¢‹ì•„í•´ì„œ ê³„ì† ë§Œë‚˜ê³  ì‹¶ì—ˆì§€ë§Œ,. ì‹œí„°ì˜ ê°œì¸ì ì¸ ì‚¬ì •ìœ¼ë¡œ ê·¸ë§Œë’€ë„¤ìš” ã…  ì—„ë§ˆì˜ ì‹œì„  5ì¼ì˜ ì§§ì€ ì‹œê°„ì˜ ë§Œë‚¨ì´ì—ˆì§€ë§Œ ê°ì‚¬í–ˆìŠµë‹ˆë‹¤ ì•½ì†ì‹œê°„ë³´ë‹¤ ì¼ì° ì˜¨ ì ë„ ìˆê³  ë§íˆ¬ëŠ” ì°¨ë¶„í•œ ë° ì•„ì´ì™€ ëŒ€í™”í•˜ë©´ ë°ê²Œ ë°”ê¿” ì˜í–ˆì–´ìš” í”¼ì•„ë…¸ë¡œ ë™ìš”ë¥¼ ì¹˜ë©´ì„œ ì¦ê²ê²Œ ë†€ì•„ì£¼ê¸°ë„ í–ˆì–´ìš” í•­ìƒ ìš´ë™ì„ í•˜ê³  ì˜¤ì‹œëŠ” ê±°ë¼ ì”»ê³  ì˜¤ ì‹œëŠ” ê±°ê² ì§€ë§Œ ìš´ë™ë³µ ì°¨ë¦¼ì˜ ë³´ìœ¡ì€ ì«´ ê¸ˆ ì‹«ì—ˆìŠµë‹ˆë‹¤ ì§€ ì•ˆì´ì™€ ì±… ì½ëŠ” ê±¸ í•œ ì ì´ ë³„ ë£¨ ì—†ì–´ ì•„ì‰½ìŠµë‹ˆë‹¤ ë³´ë‹¤ ì ê·¹ì ì¸ ë³´ìœ¡ì„ ë°°ìš° ì‹œë©´ ì¢‹ê² ì–´ìš”\n",
            "3265 : 4.160842036575476 :: ì‹œë²” ì±„ìš©ìœ¼ë¡œ ì•„ì´ë‘ í•˜ë£¨ ë™ì•ˆ ì‹œê°„ì„ ë³´ëƒˆìŠµë‹ˆë‹¤. ì–´ë¨¸ë‹˜ê³¼ í• ë¨¸ë‹˜ ë‘ ë¶„ ë‹¤ ë„ˆë¬´ ë”°ëœ»í•˜ê²Œ ë°›ì•„ ì£¼ì…”ì„œ ì•„ì´ë‘ ì¦ê²ê²Œ ë†€ ìˆ˜ ìˆì—ˆë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤ ã…ã… ìƒì˜ í›„ì— ì—°ë½ ì£¼ì‹ ë‹¤ê³  í•˜ì…¨ëŠ”ë° ì¢‹ì€ ê¸°íšŒ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤~\n",
            "3083 : 4.0934370001688904 :: ì œê°€ ë§˜ì‹œí„° ì´ìš©í•˜ë©´ì„œ ì •ë§ ì´ë ‡ê²Œ ë§Œì¡±í•œ ì ì´ ì—†ëŠ” ê²ƒ ê°™ì•„ìš”!!! ì‹œê°„ ì•½ì†ë„ ë¬¼ë¡  ì˜ ì§€ì¼œì£¼ì…¨ê³  ì•„ì´ë“¤ ì…‹ì„ ë§¡ê²¼ìŒì—ë„ ì•„ì´ ì…‹ì´ ê³ ë£¨ ì„ ìƒë‹˜ê³¼ í•¨ê»˜ í–ˆë˜ ì‹œê°„ì´ ì¦ê±°ì› ë‹¤ê³  ì´ì•¼ê¸°í•˜ë”ë¼ê³  ìš” ì„ ìƒë‹˜ê»˜ì„œ ì €í¬ ì•„ì´ë“¤ í•œ ëª… í•œ ëª… ì• ì •ì„ ê°–ê³  ì¼€ì–´í•´ì£¼ì‹  ê²Œ ëŠê»´ì ¸ ë‹¤ìŒë‚  í•˜ë£¨ ë” ë§¡ê¸°ê²Œ ë˜ì—ˆì–´ìš” ê°€ê¹Œì´ ì‚´ë©´ ìì£¼ ë¶€íƒë“œë ¸ì„ ê²ƒ ê°™ì€ë° ; ë§¤ìš° ì•„ ì‰½ìŠµë‹ˆë‹¤ ã… -ã…  ë˜ ëµ ìˆ˜ ìˆìœ¼ë©´ ì¢‹ê² ë„¤ìš”..\n",
            "60 : 4.077346975911627 :: ì±„ìš©ë˜ì–´ ëŒë´„ ì˜¤ì‹œê¸°ë¡œ í•œ ì „ë‚  ë°¤ 8:30ë¶„ì¯¤ ì—°ë½ ì™€ì„œ ê°ê¸° ê¸°ìš´ì´ ìˆìœ¼ì‹œë‹¤ê³  í•˜ì…¨ë„¤ìš”.. ëª¸ ìƒíƒœê°€ ì˜¤ í›„ë³´ë‹¨ ë‚˜ì•„ì¡Œì§€ë§Œ ì•„ì´ë“¤ì—ê²Œ ì”ë³‘ ì˜®ê¸¸ê¹Œ ê±±ì •ì´ë¼ í•˜ì…”ì„œ ì±„ìš© ì·¨ì†Œí•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤~ ëŠ¦ê²Œë¼ë„ ì—°ë½ ì£¼ì‹  ê±´ ê°ì‚¬í•˜ì§€ë§Œ.. ëª¸ ìƒíƒœê°€ ê·¸ë•Œë¶€í„° ì•ˆ ì¢‹ì•„ ì§€ìœ¼ì‹  ê±´ ì•„ë‹ í…Œê³ .. ì œê°€ ë‹¤ìŒë‚  ì•„ì´ ëŒë´ì¤„ ì‚¬ëŒì„ ê¸‰íˆ êµ¬í•˜ê¸°ì—ëŠ” ì‹œê°„ì´ ë‹¤ì†Œ ëŠ¦ì€ ê°ì´ ìˆê³  ë‹µ ì¥ ë‹¬ë¼ í•˜ì…” ì„œ ë¬¸ì ë“œë ¸ëŠ”ë° ë‹µì€ ì—†ìœ¼ì‹œë”ë¼ê³ ìš” ;;; ì—¬í•˜íŠ¼ ì»¨ë””ì…˜ ê´€ë¦¬ ì˜ í•˜ì…”ì•¼ê² ë„¤ìš” ì•„ì´ëŒ ë³¼ ë• ì²´ë ¥ì´ ê¸°ë³¸ì´ë‹ˆê¹Œìš”~~\n",
            "2110 : 4.059870809713588 :: ì±„ìš©ë˜ì–´ ì•„ì´ë¥¼ ëŒë³´ì•˜ìŠµë‹ˆë‹¤. ì €í•œí…Œ ë‘ ë²ˆ ì‹ ì²­ì„ ì£¼ì…”ì„œ ì²˜ìŒì—” ë°”ë¹ ì„œ ê±°ì ˆí–ˆë‹¤ê°€ ë˜ ì‹ ì²­ì„ ì£¼ì…”ì„œ ìŠ¤ì¼€ì¤„ì„ ë§ì¶° ë³´ë ¤ê³  ìˆ˜ë½ì„ í–ˆì–´ìš”. ë°”ë¡œ ì—°ë½ì„ ì£¼ì…¨ê³ , ê¸ˆìš”ì¼ì— ì‹œë²” ìˆ˜ì—… 1ì‹œê°„ì„ í•´ë³´ìê³  í•˜ì…¨ì–´ìš”~ ì‹œë²” ìˆ˜ì—… í›„ ìŠ¤ì¼€ì¤„ê¹Œì§€ ì¡ì•˜ëŠ”ë° ê±°ë¦¬ê°€ ë©€ë‹¤ê³  ë‹¤ë¥¸ ì„ ìƒë‹˜ì„ ì°¾ê² ë‹¤ê³  í•˜ì…¨ìŠµë‹ˆë‹¤! í”„ë¡œí•„ì— ì‚¬ëŠ” êµ¬ì™€ ë™ë„¤ê¹Œì§€ ìì„¸íˆ ì ì–´ ë‘ì—ˆëŠ”ë°, ê·¸ëŸ° ì ì€ ì¡° ê¸ˆ ì•„ì‰¬ì› ìŠµë‹ˆë‹¤ ì•„ì´ë„ ì˜ˆì˜ê³ , ë¶€ëª¨ë‹˜ê»˜ì„œë„ ì—°ë½ì´ ì•„ì£¼ ë¹ ë¥´ê³  ì¹œì ˆí•˜ì…¨ì–´ìš”~ ì €ëŠ” ì •ê¸°ì ìœ¼ë¡œ ì¼ì •í•œ ìŠ¤ì¼€ì¤„ì„ ì„ í˜¸í•˜ëŠ” í¸ì´ê³  ê·¸ëŸ¬ë‹¤ ë³´ë‹ˆ ê³  ì • ìŠ¤ì¼€ì¤„ì´ ë§ì•„ì„œ ì±„ìš©ì´ ë¶ˆë°œëœ ê²ƒ ê°™ì•„ìš”. ì¢‹ì€ ë¶„ ë§Œë‚˜ì‹œê¸¸ ë°”ë¼ìš”~\n",
            "2793 : 4.053701036818809 :: ì„ ìƒë‹˜ì´ ì € ì—†ëŠ” ë™ì•ˆ ì˜ ë†€ì•„ì£¼ì…¨ëŠ”ì§€ ì•„ì´ê°€ ì„ ìƒë‹˜ ëª» ê°€ê²Œ í•˜êµ¬ ê·¸ë˜ì„œ í•œ ì‹œê°„ ì •ë„ ë” ìˆë‹¤ê°€ ê°€ì…¨ì–´ìš”~ ë°¥ì€ ì œê°€ í•´ë†“ì€ ê±° ì±™ê²¨ì£¼ì…¨ê³ ìš”~ ë¬¼ê°ë†€ì´ë„ í•˜êµ¬ ì¥ë‚œê°ë„ ê°–ê³  ë†€ë©° ì•„ì´ì˜ ê²¬ ë“¤ì–´ì£¼ë©° ì˜ ì§€ëƒˆì—ˆë‚˜ ë´…ë‹ˆë‹¤ ì‘ê°€ë„ ë‹¦ì•„ì£¼ì…¨ê³ ìš”~???? ì°©í•˜ê³  ì˜ˆìœ ì„ ìƒë‹˜ì´ì…”ì„œ ì•„ì´ë“¤ì´ ë” ì¢‹ì•„í–ˆì„ ê²ƒ ê°™ì•„ìš”~ ì–´ì©” ìˆ˜ ì—†ì´ ì•„ì´ë“¤ë§Œ ë‘ê³  ì¼í•˜ëŸ¬ ê°€ì•¼ í•  ìƒí™©ì´ì—ˆëŠ”ë° ì˜ ëŒë´ì£¼ì…”ì„œ ë„ˆë¬´ ê°ì‚¬í–ˆìŠµë‹ˆë‹¤\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXKLAk2SNOe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a7be67b-a375-4fa1-bd71-c12dee3858f3"
      },
      "source": [
        "for i in range(3):\n",
        "  print(f'=================={i+1}ë²ˆì§¸====================')\n",
        "  keywords = keyword_extractor.summarize(test[i], topk=20)\n",
        "  for word, rank in keywords:\n",
        "      print(word, rank)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================1ë²ˆì§¸====================\n",
            "í•˜/VV 69.64618576560952\n",
            "ë˜/VV 35.1358039670359\n",
            "ì¢‹/VA 32.1107361774536\n",
            "ì—°ë½/NNG 30.531011295014977\n",
            "ì‹œê°„/NNG 22.929137049708736\n",
            "ë¶„/NNB 22.565207247408395\n",
            "ê°™/VA 17.745956844072886\n",
            "ì±„ìš©/NNG 17.58230697588734\n",
            "ìˆ/VV 17.54476093750468\n",
            "ë§/VV 16.60933009293096\n",
            "ë“œë¦¬/VV 16.310709033545425\n",
            "ì•„ì´/NNG 16.000918197881266\n",
            "ì‹œ/NNB 15.678659905661148\n",
            "ì•„ì‰½/VA 15.597821633460688\n",
            "í„°/NNB 15.529265569233008\n",
            "ë§Œë‚˜/VV 15.500843622787828\n",
            "ê²ƒ/NNB 15.19944150059622\n",
            "ì—†/VA 14.160044494308952\n",
            "ì¸í„°ë·°/NNP 12.551800315079223\n",
            "ë°”ë¼/VV 12.041957877267377\n",
            "==================2ë²ˆì§¸====================\n",
            "ì—°ë½/NNG 68.53064480202643\n",
            "í•˜/VV 36.6042609177784\n",
            "ì¢‹/VA 31.183454104570764\n",
            "ë˜/VV 26.798283734699638\n",
            "ìˆ˜ë½/NNG 24.86154221027692\n",
            "ì—†/VA 18.703968590307664\n",
            "ì˜¤/VV 16.50493124601051\n",
            "ë“œë¦¬/VV 15.493513267783275\n",
            "í„°/NNB 15.109818119243958\n",
            "ë§Œë‚˜/VV 15.048063989529226\n",
            "ì•„ì‰½/VA 14.969826866741592\n",
            "ì‹œê°„/NNG 14.499242669122124\n",
            "ë¶„/NNB 14.071351207760484\n",
            "ì‹œ/NNB 13.113364448694018\n",
            "ë°”ë¼/VV 12.57304038963731\n",
            "ë‹¤ìŒ/NNG 11.397023194240061\n",
            "ê¸°íšŒ/NNG 10.648297177674523\n",
            "ìˆ/VV 10.541441725771532\n",
            "ë‹¿/VV 9.941231321785189\n",
            "ê¸°ë‹¤ë¦¬/VV 9.505990095243167\n",
            "==================3ë²ˆì§¸====================\n",
            "í•˜/VV 78.08706970710081\n",
            "ì•„ì´/NNG 59.05540621213012\n",
            "ì‹œê°„/NNG 35.23179194855298\n",
            "ì¢‹/VA 30.061710605621567\n",
            "ë˜/VV 28.81509503864707\n",
            "ìˆ/VV 26.07369444958365\n",
            "ë¶„/NNB 22.539134852821526\n",
            "ê²ƒ/NNB 22.374852174322303\n",
            "ìˆ˜/NNB 21.06363236190035\n",
            "ëŒë³´/VV 20.8113016401538\n",
            "ê°™/VA 19.862471482829292\n",
            "ì—†/VA 18.645559359681474\n",
            "ì•„ê¸°/NNG 18.330027287968278\n",
            "ë³´/VV 18.288282180709086\n",
            "ê°ì‚¬/NNG 16.428004772159237\n",
            "ë†€/VV 15.927846497762415\n",
            "ì‹œ/NNB 15.784004016744708\n",
            "ë“œë¦¬/VV 15.010482467949714\n",
            "í„°/NNB 14.949360246147684\n",
            "ì¼/NNG 13.00214646163914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV4aVCe4-IeF",
        "outputId": "3c3181f1-848c-4ffe-f0af-a7a0ed0e7cef"
      },
      "source": [
        "TF_IDF = pd.DataFrame()\n",
        "tmp_word = []\n",
        "tmp_label = []\n",
        "tmp_score = []\n",
        "\n",
        "for i in range(3):\n",
        "  print(f'=================={i+1}ë²ˆì§¸====================')\n",
        "  keywords = keyword_extractor.summarize(test[i], topk=20)\n",
        "  for word, rank in keywords:\n",
        "    # print(word, rank)\n",
        "    tmp_label.append(i)\n",
        "    tmp_word.append(word)\n",
        "    tmp_score.append(rank)\n",
        "\n",
        "TF_IDF['word'] = tmp_word\n",
        "TF_IDF['score'] = tmp_score\n",
        "TF_IDF['label'] = tmp_label\n",
        "\n",
        "TF_IDF.to_excel('/content/TF_IDF.xlsx', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================1ë²ˆì§¸====================\n",
            "==================2ë²ˆì§¸====================\n",
            "==================3ë²ˆì§¸====================\n"
          ]
        }
      ]
    }
  ]
}