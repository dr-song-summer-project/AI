{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reviewType별 전체 TF-IDF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNdoeCmm0EtKTNyR6CPsAzL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dr-song-summer-project/AI/blob/main/Keyword%20%EC%B6%94%EC%B6%9C/reviewType%EB%B3%84_%EC%A0%84%EC%B2%B4_TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkrpg_ypM0Wr"
      },
      "source": [
        "리뷰타입 별 - 문장별 TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNbY2D4GiUe8",
        "outputId": "7f83b326-e1ef-48fe-cf6a-bcc635ef2699"
      },
      "source": [
        "!pip install konlpy\n",
        "!pip install openpyxl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (2.5.9)\n",
            "Requirement already satisfied: jdcal in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.4.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0Oo-I4Rg8Kt",
        "outputId": "7382ba72-0956-4af3-a0ca-47b4bd66a6bf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgepWG13M6ud"
      },
      "source": [
        "리뷰 타입 전체 군집 별 상위 키워드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqcdTq1HM6VY"
      },
      "source": [
        "from konlpy.tag import Komoran\n",
        "\n",
        "komoran = Komoran()\n",
        "def komoran_tokenize(sent):\n",
        "    words = komoran.pos(sent, join=True)\n",
        "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feJ7FOq_VsHe"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def pagerank(x, df=0.85, max_iter=30, bias=None):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    x : scipy.sparse.csr_matrix\n",
        "        shape = (n vertex, n vertex)\n",
        "    df : float\n",
        "        Damping factor, 0 < df < 1\n",
        "    max_iter : int\n",
        "        Maximum number of iteration\n",
        "    bias : numpy.ndarray or None\n",
        "        If None, equal bias\n",
        "    Returns\n",
        "    -------\n",
        "    R : numpy.ndarray\n",
        "        PageRank vector. shape = (n vertex, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    assert 0 < df < 1\n",
        "\n",
        "    # initialize\n",
        "    A = normalize(x, axis=0, norm='l1')\n",
        "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
        "\n",
        "    # check bias\n",
        "    if bias is None:\n",
        "        bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
        "    else:\n",
        "        bias = bias.reshape(-1,1)\n",
        "        bias = A.shape[0] * bias / bias.sum()\n",
        "        assert bias.shape[0] == A.shape[0]\n",
        "        bias = (1 - df) * bias\n",
        "\n",
        "    # iteration\n",
        "    for _ in range(max_iter):\n",
        "        R = df * (A * R) + bias\n",
        "\n",
        "    return R"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm8ksoKWVuxW"
      },
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "\n",
        "def sent_graph(sents, tokenize=None, min_count=2, min_sim=0.3,\n",
        "    similarity=None, vocab_to_idx=None, verbose=False):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    sents : list of str\n",
        "        Sentence list\n",
        "    tokenize : callable\n",
        "        tokenize(sent) return list of str\n",
        "    min_count : int\n",
        "        Minimum term frequency\n",
        "    min_sim : float\n",
        "        Minimum similarity between sentences\n",
        "    similarity : callable or str\n",
        "        similarity(s1, s2) returns float\n",
        "        s1 and s2 are list of str.\n",
        "        available similarity = [callable, 'cosine', 'textrank']\n",
        "    vocab_to_idx : dict\n",
        "        Vocabulary to index mapper.\n",
        "        If None, this function scan vocabulary first.\n",
        "    verbose : Boolean\n",
        "        If True, verbose mode on\n",
        "    Returns\n",
        "    -------\n",
        "    sentence similarity graph : scipy.sparse.csr_matrix\n",
        "        shape = (n sents, n sents)\n",
        "    \"\"\"\n",
        "\n",
        "    if vocab_to_idx is None:\n",
        "        idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
        "    else:\n",
        "        idx_to_vocab = [vocab for vocab, _ in sorted(vocab_to_idx.items(), key=lambda x:x[1])]\n",
        "\n",
        "    x = vectorize_sents(sents, tokenize, vocab_to_idx)\n",
        "    if similarity == 'cosine':\n",
        "        x = numpy_cosine_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
        "    else:\n",
        "        x = numpy_textrank_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
        "    return x\n",
        "\n",
        "def vectorize_sents(sents, tokenize, vocab_to_idx):\n",
        "    rows, cols, data = [], [], []\n",
        "    for i, sent in enumerate(sents):\n",
        "        counter = Counter(tokenize(sent))\n",
        "        for token, count in counter.items():\n",
        "            j = vocab_to_idx.get(token, -1)\n",
        "            if j == -1:\n",
        "                continue\n",
        "            rows.append(i)\n",
        "            cols.append(j)\n",
        "            data.append(count)\n",
        "    n_rows = len(sents)\n",
        "    n_cols = len(vocab_to_idx)\n",
        "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
        "\n",
        "def numpy_cosine_similarity_matrix(x, min_sim=0.3, verbose=True, batch_size=1000):\n",
        "    n_rows = x.shape[0]\n",
        "    mat = []\n",
        "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
        "        b = int(bidx * batch_size)\n",
        "        e = min(n_rows, int((bidx+1) * batch_size))\n",
        "        psim = 1 - pairwise_distances(x[b:e], x, metric='cosine')\n",
        "        rows, cols = np.where(psim >= min_sim)\n",
        "        data = psim[rows, cols]\n",
        "        mat.append(csr_matrix((data, (rows, cols)), shape=(e-b, n_rows)))\n",
        "        if verbose:\n",
        "            print('\\rcalculating cosine sentence similarity {} / {}'.format(b, n_rows), end='')\n",
        "    mat = sp.sparse.vstack(mat)\n",
        "    if verbose:\n",
        "        print('\\rcalculating cosine sentence similarity was done with {} sents'.format(n_rows))\n",
        "    return mat\n",
        "\n",
        "def numpy_textrank_similarity_matrix(x, min_sim=0.3, verbose=True, min_length=1, batch_size=1000):\n",
        "    n_rows, n_cols = x.shape\n",
        "\n",
        "    # Boolean matrix\n",
        "    rows, cols = x.nonzero()\n",
        "    data = np.ones(rows.shape[0])\n",
        "    z = csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
        "\n",
        "    # Inverse sentence length\n",
        "    size = np.asarray(x.sum(axis=1)).reshape(-1)\n",
        "    size[np.where(size <= min_length)] = 10000\n",
        "    size = np.log(size)\n",
        "\n",
        "    mat = []\n",
        "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
        "\n",
        "        # slicing\n",
        "        b = int(bidx * batch_size)\n",
        "        e = min(n_rows, int((bidx+1) * batch_size))\n",
        "\n",
        "        # dot product\n",
        "        inner = z[b:e,:] * z.transpose()\n",
        "\n",
        "        # sentence len[i,j] = size[i] + size[j]\n",
        "        norm = size[b:e].reshape(-1,1) + size.reshape(1,-1)\n",
        "        norm = norm ** (-1)\n",
        "        norm[np.where(norm == np.inf)] = 0\n",
        "\n",
        "        # normalize\n",
        "        sim = inner.multiply(norm).tocsr()\n",
        "        rows, cols = (sim >= min_sim).nonzero()\n",
        "        data = np.asarray(sim[rows, cols]).reshape(-1)\n",
        "\n",
        "        # append\n",
        "        mat.append(csr_matrix((data, (rows, cols)), shape=(e-b, n_rows)))\n",
        "\n",
        "        if verbose:\n",
        "            print('\\rcalculating textrank sentence similarity {} / {}'.format(b, n_rows), end='')\n",
        "\n",
        "    mat = sp.sparse.vstack(mat)\n",
        "    if verbose:\n",
        "        print('\\rcalculating textrank sentence similarity was done with {} sents'.format(n_rows))\n",
        "\n",
        "    return mat\n",
        "\n",
        "def graph_with_python_sim(tokens, verbose, similarity, min_sim):\n",
        "    if similarity == 'cosine':\n",
        "        similarity = cosine_sent_sim\n",
        "    elif callable(similarity):\n",
        "        similarity = similarity\n",
        "    else:\n",
        "        similarity = textrank_sent_sim\n",
        "\n",
        "    rows, cols, data = [], [], []\n",
        "    n_sents = len(tokens)\n",
        "    for i, tokens_i in enumerate(tokens):\n",
        "        if verbose and i % 1000 == 0:\n",
        "            print('\\rconstructing sentence graph {} / {} ...'.format(i, n_sents), end='')\n",
        "        for j, tokens_j in enumerate(tokens):\n",
        "            if i >= j:\n",
        "                continue\n",
        "            sim = similarity(tokens_i, tokens_j)\n",
        "            if sim < min_sim:\n",
        "                continue\n",
        "            rows.append(i)\n",
        "            cols.append(j)\n",
        "            data.append(sim)\n",
        "    if verbose:\n",
        "        print('\\rconstructing sentence graph was constructed from {} sents'.format(n_sents))\n",
        "    return csr_matrix((data, (rows, cols)), shape=(n_sents, n_sents))\n",
        "\n",
        "def textrank_sent_sim(s1, s2):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    s1, s2 : list of str\n",
        "        Tokenized sentences\n",
        "    Returns\n",
        "    -------\n",
        "    Sentence similarity : float\n",
        "        Non-negative number\n",
        "    \"\"\"\n",
        "    n1 = len(s1)\n",
        "    n2 = len(s2)\n",
        "    if (n1 <= 1) or (n2 <= 1):\n",
        "        return 0\n",
        "    common = len(set(s1).intersection(set(s2)))\n",
        "    base = math.log(n1) + math.log(n2)\n",
        "    return common / base\n",
        "\n",
        "def cosine_sent_sim(s1, s2):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    s1, s2 : list of str\n",
        "        Tokenized sentences\n",
        "    Returns\n",
        "    -------\n",
        "    Sentence similarity : float\n",
        "        Non-negative number\n",
        "    \"\"\"\n",
        "    if (not s1) or (not s2):\n",
        "        return 0\n",
        "\n",
        "    s1 = Counter(s1)\n",
        "    s2 = Counter(s2)\n",
        "    norm1 = math.sqrt(sum(v ** 2 for v in s1.values()))\n",
        "    norm2 = math.sqrt(sum(v ** 2 for v in s2.values()))\n",
        "    prod = 0\n",
        "    for k, v in s1.items():\n",
        "        prod += v * s2.get(k, 0)\n",
        "    return prod / (norm1 * norm2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1x0pLGCSG-w"
      },
      "source": [
        "import numpy as np\n",
        "class KeywordSummarizer:\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    sents : list of str\n",
        "        Sentence list\n",
        "    tokenize : callable\n",
        "        Tokenize function: tokenize(str) = list of str\n",
        "    min_count : int\n",
        "        Minumum frequency of words will be used to construct sentence graph\n",
        "    window : int\n",
        "        Word cooccurrence window size. Default is -1.\n",
        "        '-1' means there is cooccurrence between two words if the words occur in a sentence\n",
        "    min_cooccurrence : int\n",
        "        Minimum cooccurrence frequency of two words\n",
        "    vocab_to_idx : dict or None\n",
        "        Vocabulary to index mapper\n",
        "    df : float\n",
        "        PageRank damping factor\n",
        "    max_iter : int\n",
        "        Number of PageRank iterations\n",
        "    verbose : Boolean\n",
        "        If True, it shows training progress\n",
        "    \"\"\"\n",
        "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
        "        window=-1, min_cooccurrence=2, vocab_to_idx=None,\n",
        "        df=0.85, max_iter=30, verbose=False):\n",
        "\n",
        "        self.tokenize = tokenize\n",
        "        self.min_count = min_count\n",
        "        self.window = window\n",
        "        self.min_cooccurrence = min_cooccurrence\n",
        "        self.vocab_to_idx = vocab_to_idx\n",
        "        self.df = df\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if sents is not None:\n",
        "            self.train_textrank(sents)\n",
        "\n",
        "    def train_textrank(self, sents, bias=None):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        sents : list of str\n",
        "            Sentence list\n",
        "        bias : None or numpy.ndarray\n",
        "            PageRank bias term\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "\n",
        "        g, self.idx_to_vocab = word_graph(sents,\n",
        "            self.tokenize, self.min_count,self.window,\n",
        "            self.min_cooccurrence, self.vocab_to_idx, self.verbose)\n",
        "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
        "        if self.verbose:\n",
        "            print('trained TextRank. n words = {}'.format(self.R.shape[0]))\n",
        "\n",
        "    def keywords(self, topk=30):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        topk : int\n",
        "            Number of keywords selected from TextRank\n",
        "        Returns\n",
        "        -------\n",
        "        keywords : list of tuple\n",
        "            Each tuple stands for (word, rank)\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'R'):\n",
        "            raise RuntimeError('Train textrank first or use summarize function')\n",
        "        idxs = self.R.argsort()[-topk:]\n",
        "        keywords = [(self.idx_to_vocab[idx], self.R[idx]) for idx in reversed(idxs)]\n",
        "        return keywords\n",
        "\n",
        "    def summarize(self, sents, topk=30):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        sents : list of str\n",
        "            Sentence list\n",
        "        topk : int\n",
        "            Number of keywords selected from TextRank\n",
        "        Returns\n",
        "        -------\n",
        "        keywords : list of tuple\n",
        "            Each tuple stands for (word, rank)\n",
        "        \"\"\"\n",
        "\n",
        "        self.train_textrank(sents)\n",
        "        return self.keywords(topk)\n",
        "\n",
        "        \n",
        "\n",
        "class KeysentenceSummarizer:\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    sents : list of str\n",
        "        Sentence list\n",
        "    tokenize : callable\n",
        "        Tokenize function: tokenize(str) = list of str\n",
        "    min_count : int\n",
        "        Minumum frequency of words will be used to construct sentence graph\n",
        "    min_sim : float\n",
        "        Minimum similarity between sentences in sentence graph\n",
        "    similarity : str\n",
        "        available similarity = ['cosine', 'textrank']\n",
        "    vocab_to_idx : dict or None\n",
        "        Vocabulary to index mapper\n",
        "    df : float\n",
        "        PageRank damping factor\n",
        "    max_iter : int\n",
        "        Number of PageRank iterations\n",
        "    verbose : Boolean\n",
        "        If True, it shows training progress\n",
        "    \"\"\"\n",
        "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
        "        min_sim=0.3, similarity=None, vocab_to_idx=None,\n",
        "        df=0.85, max_iter=30, verbose=False):\n",
        "\n",
        "        self.tokenize = tokenize\n",
        "        self.min_count = min_count\n",
        "        self.min_sim = min_sim\n",
        "        self.similarity = similarity\n",
        "        self.vocab_to_idx = vocab_to_idx\n",
        "        self.df = df\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if sents is not None:\n",
        "            self.train_textrank(sents)\n",
        "\n",
        "    def train_textrank(self, sents, bias=None):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        sents : list of str\n",
        "            Sentence list\n",
        "        bias : None or numpy.ndarray\n",
        "            PageRank bias term\n",
        "            Shape must be (n_sents,)\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        g = sent_graph(sents, self.tokenize, self.min_count,\n",
        "            self.min_sim, self.similarity, self.vocab_to_idx, self.verbose)\n",
        "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
        "        if self.verbose:\n",
        "            print('trained TextRank. n sentences = {}'.format(self.R.shape[0]))\n",
        "\n",
        "    def summarize(self, sents, topk=30, bias=None):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        sents : list of str\n",
        "            Sentence list\n",
        "        topk : int\n",
        "            Number of key-sentences to be selected.\n",
        "        bias : None or numpy.ndarray\n",
        "            PageRank bias term\n",
        "            Shape must be (n_sents,)\n",
        "        Returns\n",
        "        -------\n",
        "        keysents : list of tuple\n",
        "            Each tuple stands for (sentence index, rank, sentence)\n",
        "        Usage\n",
        "        -----\n",
        "            >>> from textrank import KeysentenceSummarizer\n",
        "            >>> summarizer = KeysentenceSummarizer(tokenize = tokenizer, min_sim = 0.5)\n",
        "            >>> keysents = summarizer.summarize(texts, topk=30)\n",
        "        \"\"\"\n",
        "        n_sents = len(sents)\n",
        "        if isinstance(bias, np.ndarray):\n",
        "            if bias.shape != (n_sents,):\n",
        "                raise ValueError('The shape of bias must be (n_sents,) but {}'.format(bias.shape))\n",
        "        elif bias is not None:\n",
        "            raise ValueError('The type of bias must be None or numpy.ndarray but the type is {}'.format(type(bias)))\n",
        "\n",
        "        self.train_textrank(sents, bias)\n",
        "        idxs = self.R.argsort()[-topk:]\n",
        "        try:\n",
        "          keysents = [(idx, self.R[idx], sents[idx]) for idx in reversed(idxs)]\n",
        "        except:\n",
        "          print(sents)\n",
        "          keysents = False\n",
        "\n",
        "        return keysents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi3tL7kUV0WX"
      },
      "source": [
        "from collections import Counter\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def scan_vocabulary(sents, tokenize=None, min_count=2):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    sents : list of str\n",
        "        Sentence list\n",
        "    tokenize : callable\n",
        "        tokenize(str) returns list of str\n",
        "    min_count : int\n",
        "        Minumum term frequency\n",
        "    Returns\n",
        "    -------\n",
        "    idx_to_vocab : list of str\n",
        "        Vocabulary list\n",
        "    vocab_to_idx : dict\n",
        "        Vocabulary to index mapper.\n",
        "    \"\"\"\n",
        "    counter = Counter(w for sent in sents for w in tokenize(sent))\n",
        "    counter = {w:c for w,c in counter.items() if c >= min_count}\n",
        "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
        "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
        "    return idx_to_vocab, vocab_to_idx\n",
        "\n",
        "def tokenize_sents(sents, tokenize):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    sents : list of str\n",
        "        Sentence list\n",
        "    tokenize : callable\n",
        "        tokenize(sent) returns list of str (word sequence)\n",
        "    Returns피임\n",
        "    -------\n",
        "    tokenized sentence list : list of list of str\n",
        "    \"\"\"\n",
        "    return [tokenize(sent) for sent in sents]\n",
        "\n",
        "def vectorize(tokens, vocab_to_idx):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    tokens : list of list of str\n",
        "        Tokenzed sentence list\n",
        "    vocab_to_idx : dict\n",
        "        Vocabulary to index mapper\n",
        "    Returns\n",
        "    -------\n",
        "    sentence bow : scipy.sparse.csr_matrix\n",
        "        shape = (n_sents, n_terms)\n",
        "    \"\"\"\n",
        "    rows, cols, data = [], [], []\n",
        "    for i, tokens_i in enumerate(tokens):\n",
        "        for t, c in Counter(tokens_i).items():\n",
        "            j = vocab_to_idx.get(t, -1)\n",
        "            if j == -1:\n",
        "                continue\n",
        "            rows.append(i)\n",
        "            cols.append(j)\n",
        "            data.append(c)\n",
        "    n_sents = len(tokens)\n",
        "    n_terms = len(vocab_to_idx)\n",
        "    x = csr_matrix((data, (rows, cols)), shape=(n_sents, n_terms))\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2nygRz_NFEG",
        "outputId": "3fb74091-9134-4b5c-e5e0-e74fe0a966e4"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "text = '안녕하세요 반갑습니다🐶'\n",
        "print(text) \n",
        "only_BMP_pattern = re.compile(\"[\"\n",
        "        u\"\\U00010000-\\U0010FFFF\"  #BMP characters 이외\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "path = '/content/drive/My Drive/닥터송 여름 프로젝트/4. 대-스타 해결 2/데이터/unlabeled_data_excel.csv'\n",
        "df = pd.read_csv(path)\n",
        "data = pd.DataFrame.to_numpy(df)\n",
        "\n",
        "\n",
        "test = [[] for _ in range(3)]\n",
        "idx = 0\n",
        "for content in data:\n",
        "  if content[5] == 'recruitReview':\n",
        "    test[2].append(only_BMP_pattern.sub(r'', content[3]))\n",
        "  elif content[5] == 'interviewReview':\n",
        "    test[0].append(only_BMP_pattern.sub(r'', content[3]))\n",
        "  else:\n",
        "    test[1].append(only_BMP_pattern.sub(r'', content[3]))\n",
        "\n",
        "print(test[0][0], test[1][0], test[2][0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요 반갑습니다🐶\n",
            "안녕하세요 반갑습니다\n",
            "수락하시고 전화 인터뷰 진행해주셨고요. 첫째 아이 돌봄 구인 글을 보고 지원을 하였는데, 전화 인터뷰 진행하실 때 둘째 아이 방학이 다가오는 데 그때도 괜찮냐고 물어보셨지만, 둘째 아이가 방학하는 주에는 선약 돌봄 집들이 있다고 말씀드리니, 첫째 아이와 둘째 아이를 같이 돌볼 수 있는 분을 찾으신다고 하셔서 아쉽게도 뵙지 못했습니다. 아이들과 새해에도 행복하세요. 연락이 오지 않아 뵙지 못하였습니다 돌보시다가 시간이 맞지 않으셨는지 곧 그만두신 다 하셨습니다. 일하시다 불만족스러운 신 부분을 나름 조율해 드리려 했으나, 다른 조건 조율 과정 없이 바로 그만두시겠다 하셔서 워킹맘 입장에서 매우 난처했습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POZHtVe2NJ5R",
        "outputId": "e1659689-ea1a-4062-f96c-4c9904564bef"
      },
      "source": [
        "summarizer = KeysentenceSummarizer(\n",
        "    tokenize = komoran_tokenize,\n",
        "    min_sim = 0.5,\n",
        "    verbose = True\n",
        ")\n",
        "\n",
        "keysents = summarizer.summarize(test[2], topk=10)\n",
        "for sent_idx, rank, sent in keysents:  \n",
        "  print(f'{sent_idx} : {rank} :: {sent}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "calculating textrank sentence similarity was done with 3321 sents\n",
            "trained TextRank. n sentences = 3321\n",
            "1808 : 5.2517252145090225 :: 아이가 첫째 날에는 낯가림도 하고 자 다 깬지 얼마 안 돼서 잠투정이 있었던 건지 그날은 내내 기분이 좋다가도 금방 다시 엄마를 찾았는데, 둘째 날은 얼굴 보자마자 웃어주고 인사도 해줬습니다. 돌봐주는 내내 기분 좋아서 잘 웃고 떼를 쓰지도 않았습니다. 아기가 에너지가 넘치고 자기주장이 확실한 편이라 자기가 무얼 원하는지 분명하게 말합니다. 그래서 아이가 지 금 원하는 해줄 수 있어서 저는 마음이 조금 편했어요. 어머님도 굉장히 유쾌하셔서 그런지 아기도 굉장히 잘 웃고 애정을 나누는 방법을 알고 있는 것 같아요. 활동비는 제가 편하게 마지막 날에 받겠다고 했고 봉투에 감사하게 글까지 적어서 주셨어요. 활동도 미리 말씀해 주신 것 외에 따로 시키신 것도 없었고 시간도 정확하게 지켜주셨습니다~ 저는 사 정상 저녁에는 시간 내기가 어려워 계속 돌봐줄 수는 없었지만 좋은 분 만나셨으면 좋겠습니다\n",
            "904 : 4.940581320667702 :: 집에 13개월 아가와 5살 남아가 있는 집입니다. 13개월 아가는 제가 케어하고 5살 남아의 하원 및 돌봄이 2주 동안(9일) 필요해서 신청드렸습니다. 그러나 4일 동안 아이를 봐주시는 중에 2일은 10분 이상 지각하셨고 아이가 화장실을 갈 때도 저에게 맡기시더군요. 그리고 중간에 돌봄을 그만둔 가장 큰 이유는 아이 저녁 때문입니다. 돌봄 시간이 4 시에서 7시였는데 아이들 저녁 이 애매해서 13개월 아이와 5살 아이를 같이 먹이고 처음에는 시터 분도 저녁을 같이 제공해드렸습니다. 그럼 5살 아이는 저녁을 먹여주시거나 케어해 주셔야 하는데 혼자 맛있게 40여 분 동안 드시더군요. 이틀 저녁을 드렸었는데 변하지를 않아서 나머지 이틀은 챙겨드리지 않았어요. 제가 혼자 아가 둘을 케어하기가 좀 힘들더라고요. 놀이는 아이에 맞춰서 놀아주셨어요. 성격은 밝고 좋으신 듯 하나 책임감이 있어 보이지는 않았습니다. 놀이 제외하고는 평가를 좋게 드릴 수가 없을 것 같아요.\n",
            "822 : 4.756238652521562 :: 처음이라 시지만, 기본적으로 아기를 너무너무 못 보셨고... 아기에 대해 잘 모르셨어요 ㅠㅠ 5시간만 필요했는데, 어쩔 수 없이 10시간 풀타임으로 채용했지만 청소 등 도와주시기로 해서 너무나 감사했어요 하지만 아이를 돌보는 일보다 기타 가사나 본인의 휴게시간을 더 챙기시는 것 같았고 아이를 잘 못 보셔서 시터를 쓰면서도 내내 불편했네요... 가서 일을 도와주시는 것도 좋지만 시터라면 무엇보다 아기를 잘 봐주시는 게 가장 중요할 것 같아 요 시간, 위생, 안 전면에서 좀 더 신경 쓰셨으면 좋겠어요. 저희와는 잘 맞지 않았지만 워낙 착하신 분이라 다른 집에서는 잘 지 내실 수 있을 것 같아요. 그동안 고생 많으셨습니다~\n",
            "2894 : 4.670203665840344 :: 경력이 있으셔서 그런지 능숙하고 자연스럽게 아이를 잘 돌봐주셨어요. 십분 일찍 오셔서 밥 먹이기, 재우기, 목욕시키기, 놀아주기, 안 해도 되는 간단 청소 설거지까지 제가 신경 쓸 필요 없게 척척 잘 해주셔서 이사 준비하느라 정신없던 저는 매우 든든했답니다. 특히 아기가 모세 기관지염 와서 안 먹고 안자 고 보챘는데 시터님께서 오래도록 안아서 쉴 수 있게 해주시고 컨디션 회복하도록 신경 써서 물 자주 먹여주시고 밥도 끈기 있게 먹여주시고 했던 것 기억에 남습니다. 덕분에 아기 건강 잘 회복하고 이사도 무사히 했던 것 같아 감사한 마음입니다. 짧은 시간이었지만 좋은 인연이었던 것 같아 오래도록 기억에 남을 것 같고 다음에 또 뵐 기회가 있었으면 좋겠습니다\n",
            "492 : 4.303652858152414 :: 저희 아이의 눈 높이 맞게 소꿉놀이를 잘해줬고 아이가 선생님 오시는 거 너무 좋아해서 계속 만나고 싶었지만,. 시터의 개인적인 사정으로 그만뒀네요 ㅠ 엄마의 시선 5일의 짧은 시간의 만남이었지만 감사했습니다 약속시간보다 일찍 온 적도 있고 말투는 차분한 데 아이와 대화하면 밝게 바꿔 잘했어요 피아노로 동요를 치면서 즐겁게 놀아주기도 했어요 항상 운동을 하고 오시는 거라 씻고 오 시는 거겠지만 운동복 차림의 보육은 쫴 금 싫었습니다 지 안이와 책 읽는 걸 한 적이 별 루 없어 아쉽습니다 보다 적극적인 보육을 배우 시면 좋겠어요\n",
            "3265 : 4.160842036575476 :: 시범 채용으로 아이랑 하루 동안 시간을 보냈습니다. 어머님과 할머님 두 분 다 너무 따뜻하게 받아 주셔서 아이랑 즐겁게 놀 수 있었던 것 같습니다 ㅎㅎ 상의 후에 연락 주신다고 하셨는데 좋은 기회 주셔서 감사합니다~\n",
            "3083 : 4.0934370001688904 :: 제가 맘시터 이용하면서 정말 이렇게 만족한 적이 없는 것 같아요!!! 시간 약속도 물론 잘 지켜주셨고 아이들 셋을 맡겼음에도 아이 셋이 고루 선생님과 함께 했던 시간이 즐거웠다고 이야기하더라고 요 선생님께서 저희 아이들 한 명 한 명 애정을 갖고 케어해주신 게 느껴져 다음날 하루 더 맡기게 되었어요 가까이 살면 자주 부탁드렸을 것 같은데 ; 매우 아 쉽습니다 ㅠ-ㅠ 또 뵐 수 있으면 좋겠네요..\n",
            "60 : 4.077346975911627 :: 채용되어 돌봄 오시기로 한 전날 밤 8:30분쯤 연락 와서 감기 기운이 있으시다고 하셨네요.. 몸 상태가 오 후보단 나아졌지만 아이들에게 잔병 옮길까 걱정이라 하셔서 채용 취소하게 되었습니다~ 늦게라도 연락 주신 건 감사하지만.. 몸 상태가 그때부터 안 좋아 지으신 건 아닐 테고.. 제가 다음날 아이 돌봐줄 사람을 급히 구하기에는 시간이 다소 늦은 감이 있고 답 장 달라 하셔 서 문자 드렸는데 답은 없으시더라고요 ;;; 여하튼 컨디션 관리 잘 하셔야겠네요 아이돌 볼 땐 체력이 기본이니까요~~\n",
            "2110 : 4.059870809713588 :: 채용되어 아이를 돌보았습니다. 저한테 두 번 신청을 주셔서 처음엔 바빠서 거절했다가 또 신청을 주셔서 스케줄을 맞춰 보려고 수락을 했어요. 바로 연락을 주셨고, 금요일에 시범 수업 1시간을 해보자고 하셨어요~ 시범 수업 후 스케줄까지 잡았는데 거리가 멀다고 다른 선생님을 찾겠다고 하셨습니다! 프로필에 사는 구와 동네까지 자세히 적어 두었는데, 그런 점은 조 금 아쉬웠습니다 아이도 예쁘고, 부모님께서도 연락이 아주 빠르고 친절하셨어요~ 저는 정기적으로 일정한 스케줄을 선호하는 편이고 그러다 보니 고 정 스케줄이 많아서 채용이 불발된 것 같아요. 좋은 분 만나시길 바라요~\n",
            "2793 : 4.053701036818809 :: 선생님이 저 없는 동안 잘 놀아주셨는지 아이가 선생님 못 가게 하구 그래서 한 시간 정도 더 있다가 가셨어요~ 밥은 제가 해놓은 거 챙겨주셨고요~ 물감놀이도 하구 장난감도 갖고 놀며 아이의 견 들어주며 잘 지냈었나 봅니다 응가도 닦아주셨고요~???? 착하고 예쁜 선생님이셔서 아이들이 더 좋아했을 것 같아요~ 어쩔 수 없이 아이들만 두고 일하러 가야 할 상황이었는데 잘 돌봐주셔서 너무 감사했습니다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXKLAk2SNOe5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a7be67b-a375-4fa1-bd71-c12dee3858f3"
      },
      "source": [
        "for i in range(3):\n",
        "  print(f'=================={i+1}번째====================')\n",
        "  keywords = keyword_extractor.summarize(test[i], topk=20)\n",
        "  for word, rank in keywords:\n",
        "      print(word, rank)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================1번째====================\n",
            "하/VV 69.64618576560952\n",
            "되/VV 35.1358039670359\n",
            "좋/VA 32.1107361774536\n",
            "연락/NNG 30.531011295014977\n",
            "시간/NNG 22.929137049708736\n",
            "분/NNB 22.565207247408395\n",
            "같/VA 17.745956844072886\n",
            "채용/NNG 17.58230697588734\n",
            "있/VV 17.54476093750468\n",
            "맞/VV 16.60933009293096\n",
            "드리/VV 16.310709033545425\n",
            "아이/NNG 16.000918197881266\n",
            "시/NNB 15.678659905661148\n",
            "아쉽/VA 15.597821633460688\n",
            "터/NNB 15.529265569233008\n",
            "만나/VV 15.500843622787828\n",
            "것/NNB 15.19944150059622\n",
            "없/VA 14.160044494308952\n",
            "인터뷰/NNP 12.551800315079223\n",
            "바라/VV 12.041957877267377\n",
            "==================2번째====================\n",
            "연락/NNG 68.53064480202643\n",
            "하/VV 36.6042609177784\n",
            "좋/VA 31.183454104570764\n",
            "되/VV 26.798283734699638\n",
            "수락/NNG 24.86154221027692\n",
            "없/VA 18.703968590307664\n",
            "오/VV 16.50493124601051\n",
            "드리/VV 15.493513267783275\n",
            "터/NNB 15.109818119243958\n",
            "만나/VV 15.048063989529226\n",
            "아쉽/VA 14.969826866741592\n",
            "시간/NNG 14.499242669122124\n",
            "분/NNB 14.071351207760484\n",
            "시/NNB 13.113364448694018\n",
            "바라/VV 12.57304038963731\n",
            "다음/NNG 11.397023194240061\n",
            "기회/NNG 10.648297177674523\n",
            "있/VV 10.541441725771532\n",
            "닿/VV 9.941231321785189\n",
            "기다리/VV 9.505990095243167\n",
            "==================3번째====================\n",
            "하/VV 78.08706970710081\n",
            "아이/NNG 59.05540621213012\n",
            "시간/NNG 35.23179194855298\n",
            "좋/VA 30.061710605621567\n",
            "되/VV 28.81509503864707\n",
            "있/VV 26.07369444958365\n",
            "분/NNB 22.539134852821526\n",
            "것/NNB 22.374852174322303\n",
            "수/NNB 21.06363236190035\n",
            "돌보/VV 20.8113016401538\n",
            "같/VA 19.862471482829292\n",
            "없/VA 18.645559359681474\n",
            "아기/NNG 18.330027287968278\n",
            "보/VV 18.288282180709086\n",
            "감사/NNG 16.428004772159237\n",
            "놀/VV 15.927846497762415\n",
            "시/NNB 15.784004016744708\n",
            "드리/VV 15.010482467949714\n",
            "터/NNB 14.949360246147684\n",
            "일/NNG 13.00214646163914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV4aVCe4-IeF",
        "outputId": "3c3181f1-848c-4ffe-f0af-a7a0ed0e7cef"
      },
      "source": [
        "TF_IDF = pd.DataFrame()\n",
        "tmp_word = []\n",
        "tmp_label = []\n",
        "tmp_score = []\n",
        "\n",
        "for i in range(3):\n",
        "  print(f'=================={i+1}번째====================')\n",
        "  keywords = keyword_extractor.summarize(test[i], topk=20)\n",
        "  for word, rank in keywords:\n",
        "    # print(word, rank)\n",
        "    tmp_label.append(i)\n",
        "    tmp_word.append(word)\n",
        "    tmp_score.append(rank)\n",
        "\n",
        "TF_IDF['word'] = tmp_word\n",
        "TF_IDF['score'] = tmp_score\n",
        "TF_IDF['label'] = tmp_label\n",
        "\n",
        "TF_IDF.to_excel('/content/TF_IDF.xlsx', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================1번째====================\n",
            "==================2번째====================\n",
            "==================3번째====================\n"
          ]
        }
      ]
    }
  ]
}